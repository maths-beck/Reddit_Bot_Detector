{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac54baae-01c5-4540-a290-ac2937ffc689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API do Google AI configurada com sucesso.\n",
      "‚úÖ Todas as bibliotecas foram importadas e a configura√ß√£o foi conclu√≠da.\n"
     ]
    }
   ],
   "source": [
    "# Instala todas as bibliotecas necess√°rias, incluindo a do Google AI\n",
    "!pip install praw pandas numpy scipy scikit-learn vadersentiment nltk google-generativeai -q\n",
    "\n",
    "# Importa√ß√µes gerais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "import praw\n",
    "import json\n",
    "\n",
    "# Importa√ß√µes para An√°lise de Texto (PLN)\n",
    "import nltk\n",
    "from scipy.stats import entropy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Importa√ß√µes do Google AI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Baixa diretamente os pacotes de dados necess√°rios para o NLTK.\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# --- FUN√á√ÉO DE CONFIGURA√á√ÉO DA GOOGLE AI API KEY ---\n",
    "# Esta fun√ß√£o recebe a chave da API como um par√¢metro e configura a biblioteca.\n",
    "def configure_google_ai(api_key):\n",
    "    \"\"\"\n",
    "    Configura a API do Google AI com a chave fornecida.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Sua chave de API do Google AI.\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        print(\"‚ö†Ô∏è Chave de API n√£o fornecida. A funcionalidade do Google AI estar√° desativada.\")\n",
    "        return\n",
    "    try:\n",
    "        genai.configure(api_key=api_key)\n",
    "        print(\"‚úÖ API do Google AI configurada com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ocorreu um erro ao configurar a API do Google AI: {e}\")\n",
    "\n",
    "# --- COMO USAR ---\n",
    "# 1. Substitua \"SUA_CHAVE_DE_API_AQUI\" pela sua chave de API real.\n",
    "# 2. Execute a c√©lula.\n",
    "\n",
    "# Exemplo de uso:\n",
    "# Coloque sua chave de API diretamente na vari√°vel abaixo.\n",
    "YOUR_GOOGLE_API_KEY = \"AIzaSyDV6VITxV5WwCgoKAh3nSlXhk1r4ZWZhSU\" \n",
    "\n",
    "# Chama a fun√ß√£o para configurar a API\n",
    "configure_google_ai(YOUR_GOOGLE_API_KEY)\n",
    "\n",
    "print(\"‚úÖ Todas as bibliotecas foram importadas e a configura√ß√£o foi conclu√≠da.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc40f42-459f-49ea-8483-aae8a6273e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replace these placeholders with your credentials ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"a9IG6BCyE9K5Il_1fbqszA\",\n",
    "        client_secret=\"NZtv0dtUDP6mLZJJ8xQteyFvzfBdvw\",\n",
    "    user_agent=\"SubredditAnalyzer/1.0 by u/YourUsername\"\n",
    ")\n",
    "\n",
    "# Check if the authentication is read-only or successful\n",
    "if reddit.read_only:\n",
    "    print(\"‚ö†  Warning: Authenticated in read-only mode. Check your credentials.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Authenticated successfully as u/{reddit.user.me()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c922610-b9db-4522-8d37-286f492d6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = 'user_database.csv'\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Carrega o banco de dados de caracter√≠sticas do usu√°rio ou o cria.\"\"\"\n",
    "    try:\n",
    "        db = pd.read_csv(DATABASE_FILE)\n",
    "    except FileNotFoundError:\n",
    "        db = pd.DataFrame(columns=[\n",
    "            'account_age_days', 'karma_ratio', 'avg_time_between_posts_sec',\n",
    "            'subreddit_entropy', 'username_is_pattern', 'comment_length_variance',\n",
    "            'submission_comment_ratio', 'link_in_comment_ratio',\n",
    "            'sentiment_avg', 'sentiment_variance', 'comment_similarity_avg',\n",
    "            'exact_duplicate_ratio', 'is_bot'\n",
    "        ])\n",
    "    return db\n",
    "\n",
    "def analyze_user_live(username, reddit_instance):\n",
    "    \"\"\"Analisa um usu√°rio com um conjunto completo de m√©tricas num√©ricas.\"\"\"\n",
    "    # ... esta fun√ß√£o permanece exatamente a mesma da vers√£o anterior ...\n",
    "    try:\n",
    "        user = reddit_instance.redditor(username)\n",
    "        _ = user.id\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"N√£o foi poss√≠vel encontrar ou acessar o usu√°rio '{username}'. Erro: {e}\"}\n",
    "    comments = list(user.comments.new(limit=100))\n",
    "    submissions = list(user.submissions.new(limit=50))\n",
    "    activities = sorted(comments + submissions, key=lambda x: x.created_utc, reverse=True)\n",
    "    if len(comments) < 5:\n",
    "        # Este retorno de erro √© para a an√°lise de m√©tricas; a an√°lise do LLM tem sua pr√≥pria l√≥gica.\n",
    "        # Mantemos isso para garantir que as m√©tricas sejam baseadas em dados suficientes.\n",
    "        # No entanto, se o objetivo √© analisar mesmo sem coment√°rios, podemos ajustar aqui tamb√©m.\n",
    "        # Por enquanto, a l√≥gica principal tratar√° um usu√°rio sem coment√°rios.\n",
    "        pass # Permite a continua√ß√£o para que o LLM possa analisar apenas as m√©tricas de perfil.\n",
    "\n",
    "    created_date = datetime.fromtimestamp(user.created_utc)\n",
    "    account_age_days = (datetime.now() - created_date).days\n",
    "    karma_ratio = user.comment_karma / (user.link_karma + 1) if user.link_karma > 0 else user.comment_karma\n",
    "    timestamps = [act.created_utc for act in activities]\n",
    "    time_deltas = np.diff(timestamps)\n",
    "    avg_time_between_posts_sec = -np.mean(time_deltas) if len(time_deltas) > 0 else 0\n",
    "    subreddits = [act.subreddit.display_name for act in activities if hasattr(act, 'subreddit')]\n",
    "    counts = pd.Series(subreddits).value_counts()\n",
    "    subreddit_entropy = entropy(counts) if not counts.empty else 0\n",
    "    pattern = r\"[a-zA-Z]+[-_][a-zA-Z]+[0-9]{2,}\"\n",
    "    username_is_pattern = 1 if re.search(pattern, username) else 0\n",
    "    comment_lengths = [len(c.body) for c in comments]\n",
    "    comment_length_variance = np.var(comment_lengths) if comment_lengths else 0\n",
    "    submission_comment_ratio = len(submissions) / (len(comments) + 1)\n",
    "    link_count = sum(1 for c in comments if 'http' in c.body.lower())\n",
    "    link_in_comment_ratio = link_count / len(comments) if comments else 0\n",
    "    comment_bodies = [c.body for c in comments]\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = [analyzer.polarity_scores(comment)['compound'] for comment in comment_bodies]\n",
    "    sentiment_avg = np.mean(sentiment_scores) if sentiment_scores else 0\n",
    "    sentiment_variance = np.var(sentiment_scores) if sentiment_scores else 0\n",
    "    comment_similarity_avg = 0.0\n",
    "    if len(comment_bodies) > 1:\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english').fit_transform(comment_bodies)\n",
    "            cosine_matrix = cosine_similarity(vectorizer)\n",
    "            upper_triangle_indices = np.triu_indices_from(cosine_matrix, k=1)\n",
    "            if upper_triangle_indices[0].size > 0:\n",
    "                comment_similarity_avg = np.mean(cosine_matrix[upper_triangle_indices])\n",
    "        except ValueError:\n",
    "            comment_similarity_avg = 0.0\n",
    "    exact_duplicate_ratio = 0.0\n",
    "    if len(comment_bodies) > 0:\n",
    "        num_unique_comments = len(set(comment_bodies))\n",
    "        exact_duplicate_ratio = (len(comment_bodies) - num_unique_comments) / len(comment_bodies)\n",
    "    features = {\n",
    "        'account_age_days': account_age_days, 'karma_ratio': karma_ratio,\n",
    "        'avg_time_between_posts_sec': avg_time_between_posts_sec, 'subreddit_entropy': subreddit_entropy,\n",
    "        'username_is_pattern': username_is_pattern, 'comment_length_variance': comment_length_variance,\n",
    "        'submission_comment_ratio': submission_comment_ratio, 'link_in_comment_ratio': link_in_comment_ratio,\n",
    "        'sentiment_avg': sentiment_avg, 'sentiment_variance': sentiment_variance,\n",
    "        'comment_similarity_avg': comment_similarity_avg, 'exact_duplicate_ratio': exact_duplicate_ratio,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "# --- FUN√á√ÉO ATUALIZADA ---\n",
    "def analyze_with_llm(username, reddit_instance, metrics_data, model_name=\"gemini-1.5-flash\"):\n",
    "    \"\"\"\n",
    "    Usa um LLM (Gemini) para analisar os dados de um usu√°rio.\n",
    "    Se o usu√°rio tiver coment√°rios, a an√°lise √© h√≠brida (m√©tricas + texto).\n",
    "    Se n√£o tiver coment√°rios, a an√°lise usa apenas as m√©tricas num√©ricas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user = reddit_instance.redditor(username)\n",
    "        # Tenta buscar os coment√°rios, mas n√£o gera erro imediato se n√£o houver\n",
    "        comments = list(user.comments.new(limit=25))\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"N√£o foi poss√≠vel buscar os dados do usu√°rio: {e}\"}\n",
    "\n",
    "    # --- L√ìGICA MODIFICADA ---\n",
    "    # Decide qual prompt usar com base na exist√™ncia de coment√°rios.\n",
    "\n",
    "    if len(comments) < 5:\n",
    "        # --- PROMPT 1: APENAS M√âTRICAS (QUANDO N√ÉO H√Å COMENT√ÅRIOS) ---\n",
    "        # Adicionei mais algumas m√©tricas para dar mais contexto ao LLM\n",
    "        prompt = f\"\"\"\n",
    "        Voc√™ √© um analista especialista em detec√ß√£o de bots na plataforma Reddit. Sua tarefa √© fazer uma\n",
    "        an√°lise do usu√°rio u/{username} baseada APENAS nos seus dados quantitativos, pois\n",
    "        ele n√£o possui coment√°rios recentes suficientes para uma an√°lise de texto.\n",
    "\n",
    "        --- DADOS QUANTITATIVOS ---\n",
    "        * Idade da Conta (dias): {metrics_data.get('account_age_days', 'N/A')} (Contas muito novas s√£o suspeitas)\n",
    "        * Propor√ß√£o de Karma (coment√°rio/link): {metrics_data.get('karma_ratio', 0.0):.2f} (Valores extremos podem ser suspeitos)\n",
    "        * Taxa de Duplica√ß√£o Exata de Coment√°rios: {metrics_data.get('exact_duplicate_ratio', 0.0):.2f} (Uma taxa > 0.25 √© forte sinal de bot)\n",
    "        * Tempo M√©dio entre Atividades (segundos): {metrics_data.get('avg_time_between_posts_sec', 0.0):.2f} (Valores muito baixos podem indicar automa√ß√£o)\n",
    "        * Entropia de Subreddits: {metrics_data.get('subreddit_entropy', 0.0):.2f} (Valores muito baixos indicam atividade concentrada)\n",
    "        * Vari√¢ncia do Tamanho dos Coment√°rios: {metrics_data.get('comment_length_variance', 0.0):.2f} (Pr√≥ximo de zero indica coment√°rios de tamanho similar)\n",
    "        ---\n",
    "\n",
    "        Com base APENAS nos dados quantitativos acima, fa√ßa sua avalia√ß√£o final.\n",
    "\n",
    "        Responda estritamente no formato JSON a seguir, sem texto adicional:\n",
    "        {{\n",
    "          \"veredicto\": \"bot\" | \"humano\",\n",
    "          \"confianca\": <um n√∫mero de 0.0 a 1.0>,\n",
    "          \"justificativa\": \"<uma an√°lise concisa de 2-3 frases explicando sua decis√£o com base APENAS nos dados num√©ricos. Mencione a aus√™ncia de coment√°rios.>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # --- PROMPT 2: AN√ÅLISE H√çBRIDA (O SEU PROMPT ORIGINAL) ---\n",
    "        comment_list_for_prompt = \"\\\\n\".join([f\"{i+1}. {c.body.strip()}\" for i, c in enumerate(comments)])\n",
    "        prompt = f\"\"\"\n",
    "        Voc√™ √© um analista especialista em detec√ß√£o de bots na plataforma Reddit. Sua tarefa √© fazer uma\n",
    "        an√°lise h√≠brida, considerando tanto os dados quantitativos quanto o conte√∫do dos coment√°rios\n",
    "        recentes do usu√°rio u/{username}.\n",
    "\n",
    "        --- DADOS QUANTITATIVOS ---\\n\n",
    "        * Idade da Conta (dias): {metrics_data.get('account_age_days', 'N/A')} (Contas muito novas s√£o mais suspeitas)\\n\n",
    "        * Propor√ß√£o de Karma (coment√°rio/link): {metrics_data.get('karma_ratio', 0.0):.2f} (Valores extremos podem ser suspeitos)\\n\n",
    "        * Taxa de Duplica√ß√£o Exata de Coment√°rios: {metrics_data.get('exact_duplicate_ratio', 0.0):.2f} (Uma taxa acima de 0.25 √© um forte sinal de bot)\n",
    "\n",
    "        --- COMENT√ÅRIOS RECENTES ({len(comments)}) ---\\n\n",
    "        {comment_list_for_prompt}\\n\n",
    "        ---\n",
    "\n",
    "        Com base em TODOS os dados acima (quantitativos e textuais), fa√ßa sua avalia√ß√£o final.\n",
    "        Considere como os dados num√©ricos refor√ßam ou contradizem a an√°lise do texto.\n",
    "\n",
    "        Responda estritamente no formato JSON a seguir, sem texto adicional:\n",
    "        {{\n",
    "          \"veredicto\": \"bot\" | \"humano\",\n",
    "          \"confianca\": <um n√∫mero de 0.0 a 1.0>,\n",
    "          \"justificativa\": \"<uma an√°lise concisa de 2-3 frases explicando sua decis√£o com base nos dados e nos coment√°rios>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "    # O restante da fun√ß√£o permanece igual\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        cleaned_response = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        return json.loads(cleaned_response)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Erro ao analisar com o LLM: {e}\", \"raw_response\": response.text if 'response' in locals() else \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1997129-453d-44f7-a6d0-aacf2264aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Banco de dados carregado com 26 entradas.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Digite a URL do perfil do usu√°rio para analisar (ou 'quit' para sair):  https://www.reddit.com/user/twitch_and_shock/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì° 1/2: Calculando m√©tricas num√©ricas para u/twitch_and_shock...\n",
      "\n",
      "--- Resultados da An√°lise de M√©tricas ---\n",
      "  - account_age_days: 1522.0000\n",
      "  - karma_ratio: 2.6310\n",
      "  - avg_time_between_posts_sec: 775919.1132\n",
      "  - subreddit_entropy: 2.3666\n",
      "  - username_is_pattern: 0.0000\n",
      "  - comment_length_variance: 83629.1379\n",
      "  - submission_comment_ratio: 0.0693\n",
      "  - link_in_comment_ratio: 0.0300\n",
      "  - sentiment_avg: 0.2016\n",
      "  - sentiment_variance: 0.1817\n",
      "  - comment_similarity_avg: 0.0152\n",
      "  - exact_duplicate_ratio: 0.0000\n",
      "\n",
      "üß† 2/2: Solicitando an√°lise do LLM para u/twitch_and_shock...\n",
      "\n",
      "--- Veredito do LLM (An√°lise H√≠brida) ---\n",
      "  - Veredito: HUMANO\n",
      "  - Confian√ßa: 0.80\n",
      "  - Justificativa: Apesar da idade da conta ser relativamente alta e a propor√ß√£o de karma estar acima da m√©dia, a aus√™ncia de duplica√ß√£o de coment√°rios e o conte√∫do consistente e t√©cnico dos coment√°rios sugerem um usu√°rio humano.  Os coment√°rios demonstram conhecimento espec√≠fico de programa√ß√£o e ferramentas de desenvolvimento, inconsistente com um bot padr√£o.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Com base em TUDO, qual √© o seu veredito final: bot (b) ou humano (h)?  h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entrada adicionada! O banco de dados agora tem 27 entradas.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Digite a URL do perfil do usu√°rio para analisar (ou 'quit' para sair):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Banco de dados salvo com sucesso em 'user_database.csv'.\n"
     ]
    }
   ],
   "source": [
    "user_db = load_database()\n",
    "print(f\"üíæ Banco de dados carregado com {len(user_db)} entradas.\")\n",
    "\n",
    "while True:\n",
    "    url = input(\"Digite a URL do perfil do usu√°rio para analisar (ou 'quit' para sair): \")\n",
    "    if url.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    match = re.search(r\"/user/([A-Za-z0-9_-]+)\", url)\n",
    "    if not match:\n",
    "        print(\"‚ùå Formato de URL inv√°lido. Use o formato: https://www.reddit.com/user/username/\")\n",
    "        continue\n",
    "    username = match.group(1)\n",
    "\n",
    "    # --- Etapa 1: An√°lise de M√©tricas ---\n",
    "    print(f\"\\nüì° 1/2: Calculando m√©tricas num√©ricas para u/{username}...\")\n",
    "    features = analyze_user_live(username, reddit)\n",
    "    if \"error\" in features:\n",
    "        print(f\"‚ùå Erro na an√°lise de m√©tricas: {features['error']}\")\n",
    "        continue\n",
    "    print(\"\\n--- Resultados da An√°lise de M√©tricas ---\")\n",
    "    for key, value in features.items():\n",
    "        print(f\"  - {key}: {value:.4f}\")\n",
    "\n",
    "    # --- Etapa 2: An√°lise com LLM (usando as m√©tricas da Etapa 1) ---\n",
    "    print(f\"\\nüß† 2/2: Solicitando an√°lise do LLM para u/{username}...\")\n",
    "    # Passa o dicion√°rio 'features' para a fun√ß√£o do LLM\n",
    "    llm_analysis = analyze_with_llm(username, reddit, features)\n",
    "    print(\"\\n--- Veredito do LLM (An√°lise H√≠brida) ---\")\n",
    "    if \"error\" in llm_analysis:\n",
    "        print(f\"‚ùå Erro na an√°lise do LLM: {llm_analysis['error']}\")\n",
    "        if 'raw_response' in llm_analysis:\n",
    "            print(f\"   Resposta Bruta: {llm_analysis['raw_response']}\")\n",
    "    else:\n",
    "        print(f\"  - Veredito: {llm_analysis.get('veredicto', 'N/A').upper()}\")\n",
    "        print(f\"  - Confian√ßa: {llm_analysis.get('confianca', 0.0):.2f}\")\n",
    "        print(f\"  - Justificativa: {llm_analysis.get('justificativa', 'N/A')}\")\n",
    "\n",
    "    # --- Etapa 3: Coleta do seu r√≥tulo final ---\n",
    "    label = ''\n",
    "    while label not in ['b', 'h']:\n",
    "        label = input(\"\\n--> Com base em TUDO, qual √© o seu veredito final: bot (b) ou humano (h)? \").lower()\n",
    "\n",
    "    features['is_bot'] = 1 if label == 'b' else 0\n",
    "    new_entry = pd.DataFrame([features])\n",
    "    user_db = pd.concat([user_db, new_entry], ignore_index=True)\n",
    "    print(f\"‚úÖ Entrada adicionada! O banco de dados agora tem {len(user_db)} entradas.\\n\")\n",
    "\n",
    "# Salva o banco de dados final no arquivo CSV\n",
    "user_db.to_csv(DATABASE_FILE, index=False)\n",
    "print(f\"\\nüíæ Banco de dados salvo com sucesso em '{DATABASE_FILE}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81646bc-0ed6-4361-a142-14eb283aae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Visualiza√ß√£o do Banco de Dados (26 entradas) ---\n",
      "\n",
      "--- Primeiras 5 Entradas ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_age_days</th>\n",
       "      <th>karma_ratio</th>\n",
       "      <th>avg_time_between_posts_sec</th>\n",
       "      <th>subreddit_entropy</th>\n",
       "      <th>username_is_pattern</th>\n",
       "      <th>comment_length_variance</th>\n",
       "      <th>submission_comment_ratio</th>\n",
       "      <th>link_in_comment_ratio</th>\n",
       "      <th>is_bot</th>\n",
       "      <th>sentiment_avg</th>\n",
       "      <th>sentiment_variance</th>\n",
       "      <th>comment_similarity_avg</th>\n",
       "      <th>exact_duplicate_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>2.759855e+04</td>\n",
       "      <td>1.860609</td>\n",
       "      <td>0</td>\n",
       "      <td>4.651600</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166</td>\n",
       "      <td>0.045242</td>\n",
       "      <td>1.394641e+04</td>\n",
       "      <td>2.997718</td>\n",
       "      <td>0</td>\n",
       "      <td>2549.250000</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1850</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>2.147339e+06</td>\n",
       "      <td>2.557894</td>\n",
       "      <td>1</td>\n",
       "      <td>3154.016529</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>405</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>3.007383e+05</td>\n",
       "      <td>1.578693</td>\n",
       "      <td>0</td>\n",
       "      <td>796.372449</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>405</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>3.007383e+05</td>\n",
       "      <td>1.578693</td>\n",
       "      <td>0</td>\n",
       "      <td>796.372449</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266357</td>\n",
       "      <td>0.239542</td>\n",
       "      <td>0.035965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_age_days  karma_ratio  avg_time_between_posts_sec  \\\n",
       "0               188     0.002024                2.759855e+04   \n",
       "1               166     0.045242                1.394641e+04   \n",
       "2              1850     0.291667                2.147339e+06   \n",
       "3               405     0.004955                3.007383e+05   \n",
       "4               405     0.004955                3.007383e+05   \n",
       "\n",
       "   subreddit_entropy  username_is_pattern  comment_length_variance  \\\n",
       "0           1.860609                    0                 4.651600   \n",
       "1           2.997718                    0              2549.250000   \n",
       "2           2.557894                    1              3154.016529   \n",
       "3           1.578693                    0               796.372449   \n",
       "4           1.578693                    0               796.372449   \n",
       "\n",
       "   submission_comment_ratio  link_in_comment_ratio  is_bot  sentiment_avg  \\\n",
       "0                  0.980392               0.000000       1            NaN   \n",
       "1                  0.980392               0.060000       1            NaN   \n",
       "2                  0.173913               0.000000       0            NaN   \n",
       "3                  3.333333               0.142857       0            NaN   \n",
       "4                  3.333333               0.142857       0       0.266357   \n",
       "\n",
       "   sentiment_variance  comment_similarity_avg  exact_duplicate_ratio  \n",
       "0                 NaN                     NaN                    NaN  \n",
       "1                 NaN                     NaN                    NaN  \n",
       "2                 NaN                     NaN                    NaN  \n",
       "3                 NaN                     NaN                    NaN  \n",
       "4            0.239542                0.035965                    NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- √öltimas 5 Entradas ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_age_days</th>\n",
       "      <th>karma_ratio</th>\n",
       "      <th>avg_time_between_posts_sec</th>\n",
       "      <th>subreddit_entropy</th>\n",
       "      <th>username_is_pattern</th>\n",
       "      <th>comment_length_variance</th>\n",
       "      <th>submission_comment_ratio</th>\n",
       "      <th>link_in_comment_ratio</th>\n",
       "      <th>is_bot</th>\n",
       "      <th>sentiment_avg</th>\n",
       "      <th>sentiment_variance</th>\n",
       "      <th>comment_similarity_avg</th>\n",
       "      <th>exact_duplicate_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>188</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>46695.429530</td>\n",
       "      <td>2.066065</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5075</td>\n",
       "      <td>0.49505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1833</td>\n",
       "      <td>42.130999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>166</td>\n",
       "      <td>0.044640</td>\n",
       "      <td>11772.966443</td>\n",
       "      <td>2.838560</td>\n",
       "      <td>0</td>\n",
       "      <td>3952.4224</td>\n",
       "      <td>0.49505</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.334498</td>\n",
       "      <td>0.139122</td>\n",
       "      <td>0.016795</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>188</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>46695.429530</td>\n",
       "      <td>2.066065</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5075</td>\n",
       "      <td>0.49505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>188</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>46695.429530</td>\n",
       "      <td>2.066065</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5075</td>\n",
       "      <td>0.49505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    account_age_days  karma_ratio  avg_time_between_posts_sec  \\\n",
       "21               188     0.002024                46695.429530   \n",
       "22              1833    42.130999                    0.000000   \n",
       "23               166     0.044640                11772.966443   \n",
       "24               188     0.002024                46695.429530   \n",
       "25               188     0.002024                46695.429530   \n",
       "\n",
       "    subreddit_entropy  username_is_pattern  comment_length_variance  \\\n",
       "21           2.066065                    0                   3.5075   \n",
       "22           0.000000                    0                   0.0000   \n",
       "23           2.838560                    0                3952.4224   \n",
       "24           2.066065                    0                   3.5075   \n",
       "25           2.066065                    0                   3.5075   \n",
       "\n",
       "    submission_comment_ratio  link_in_comment_ratio  is_bot  sentiment_avg  \\\n",
       "21                   0.49505                   0.00       1       0.000000   \n",
       "22                   0.00000                   0.00       1       0.000000   \n",
       "23                   0.49505                   0.14       1       0.334498   \n",
       "24                   0.49505                   0.00       1       0.000000   \n",
       "25                   0.49505                   0.00       1       0.000000   \n",
       "\n",
       "    sentiment_variance  comment_similarity_avg  exact_duplicate_ratio  \n",
       "21            0.000000                0.300000                   0.92  \n",
       "22            0.000000                0.000000                   0.00  \n",
       "23            0.139122                0.016795                   0.17  \n",
       "24            0.000000                0.300000                   0.92  \n",
       "25            0.000000                0.300000                   0.92  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    my_database = pd.read_csv(DATABASE_FILE)\n",
    "    print(f\"--- Visualiza√ß√£o do Banco de Dados ({len(my_database)} entradas) ---\")\n",
    "    \n",
    "    # Exibe o DataFrame inteiro se n√£o for muito grande, ou apenas o in√≠cio e o fim.\n",
    "    if len(my_database) < 20:\n",
    "        display(my_database)\n",
    "    else:\n",
    "        print(\"\\n--- Primeiras 5 Entradas ---\")\n",
    "        display(my_database.head())\n",
    "        print(\"\\n--- √öltimas 5 Entradas ---\")\n",
    "        display(my_database.tail())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"O arquivo de banco de dados '{DATABASE_FILE}' ainda n√£o foi criado.\")\n",
    "    print(\"Execute a c√©lula de coleta de dados pelo menos uma vez para cri√°-lo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470efeff-ff38-4003-873c-be49840db63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ffac1-6a08-4e63-881e-a8997210a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [21 lines of output]\n",
      "  + C:\\Users\\rrdki\\Documents\\Anaconda\\doc\\python.exe C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\vendored-meson\\meson\\meson.py setup C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\.mesonpy-md7m7zul -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\.mesonpy-md7m7zul\\meson-python-native-file.ini\n",
      "  The Meson build system\n",
      "  Version: 1.2.99\n",
      "  Source dir: C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\n",
      "  Build dir: C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\.mesonpy-md7m7zul\n",
      "  Build type: native build\n",
      "  Project name: NumPy\n",
      "  Project version: 1.26.4\n",
      "  WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "  \n",
      "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "  The following exception(s) were encountered:\n",
      "  Running `icl \"\"` gave \"[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\"\n",
      "  Running `cl /?` gave \"[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\"\n",
      "  Running `cc --version` gave \"[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\"\n",
      "  Running `gcc --version` gave \"[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\"\n",
      "  Running `clang --version` gave \"[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\"\n",
      "  Running `clang-cl /?` gave \"[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\"\n",
      "  Running `pgcc --version` gave \"[WinError 2] O sistema n√£o pode encontrar o arquivo especificado\"\n",
      "  \n",
      "  A full log can be found at C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\.mesonpy-md7m7zul\\meson-logs\\meson-log.txt\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://ce689b1746565ccbfe.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ce689b1746565ccbfe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. SETUP: INSTALL LIBRARIES ---\n",
    "# This cell installs all necessary packages, including Gradio for the UI.\n",
    "!pip install praw pandas numpy scipy scikit-learn vadersentiment nltk google-generativeai \"gradio<4.0\" -q\n",
    "\n",
    "# --- 2. IMPORTS AND CORE LOGIC ---\n",
    "# This cell contains all your original backend functions without modification.\n",
    "\n",
    "# Importa√ß√µes gerais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "import praw\n",
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "# Importa√ß√µes para An√°lise de Texto (PLN)\n",
    "import nltk\n",
    "from scipy.stats import entropy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Importa√ß√µes do Google AI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Baixa pacotes de dados do NLTK.\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Vari√°veis globais para inst√¢ncias e banco de dados\n",
    "reddit_instance = None\n",
    "user_db = None\n",
    "DATABASE_FILE = 'user_database.csv'\n",
    "is_configured = False\n",
    "\n",
    "# --- Fun√ß√µes de An√°lise (Seu c√≥digo original) ---\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Carrega o banco de dados de caracter√≠sticas do usu√°rio ou o cria.\"\"\"\n",
    "    try:\n",
    "        db = pd.read_csv(DATABASE_FILE)\n",
    "    except FileNotFoundError:\n",
    "        db = pd.DataFrame(columns=[\n",
    "            'account_age_days', 'karma_ratio', 'avg_time_between_posts_sec',\n",
    "            'subreddit_entropy', 'username_is_pattern', 'comment_length_variance',\n",
    "            'submission_comment_ratio', 'link_in_comment_ratio',\n",
    "            'sentiment_avg', 'sentiment_variance', 'comment_similarity_avg',\n",
    "            'exact_duplicate_ratio', 'is_bot'\n",
    "        ])\n",
    "    return db\n",
    "\n",
    "def analyze_user_live(username, reddit_instance):\n",
    "    \"\"\"Analisa um usu√°rio com um conjunto completo de m√©tricas num√©ricas.\"\"\"\n",
    "    try:\n",
    "        user = reddit_instance.redditor(username)\n",
    "        _ = user.id\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"N√£o foi poss√≠vel encontrar ou acessar o usu√°rio '{username}'. Erro: {e}\"}\n",
    "    comments = list(user.comments.new(limit=100))\n",
    "    submissions = list(user.submissions.new(limit=50))\n",
    "    activities = sorted(comments + submissions, key=lambda x: x.created_utc, reverse=True)\n",
    "    \n",
    "    created_date = datetime.fromtimestamp(user.created_utc)\n",
    "    account_age_days = (datetime.now() - created_date).days\n",
    "    karma_ratio = user.comment_karma / (user.link_karma + 1) if user.link_karma > 0 else user.comment_karma\n",
    "    timestamps = [act.created_utc for act in activities]\n",
    "    time_deltas = np.diff(timestamps)\n",
    "    avg_time_between_posts_sec = -np.mean(time_deltas) if len(time_deltas) > 0 else 0\n",
    "    subreddits = [act.subreddit.display_name for act in activities if hasattr(act, 'subreddit')]\n",
    "    counts = pd.Series(subreddits).value_counts()\n",
    "    subreddit_entropy = entropy(counts) if not counts.empty else 0\n",
    "    pattern = r\"[a-zA-Z]+[-_][a-zA-Z]+[0-9]{2,}\"\n",
    "    username_is_pattern = 1 if re.search(pattern, username) else 0\n",
    "    comment_lengths = [len(c.body) for c in comments]\n",
    "    comment_length_variance = np.var(comment_lengths) if comment_lengths else 0\n",
    "    submission_comment_ratio = len(submissions) / (len(comments) + 1)\n",
    "    link_count = sum(1 for c in comments if 'http' in c.body.lower())\n",
    "    link_in_comment_ratio = link_count / len(comments) if comments else 0\n",
    "    comment_bodies = [c.body for c in comments]\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = [analyzer.polarity_scores(comment)['compound'] for comment in comment_bodies]\n",
    "    sentiment_avg = np.mean(sentiment_scores) if sentiment_scores else 0\n",
    "    sentiment_variance = np.var(sentiment_scores) if sentiment_scores else 0\n",
    "    comment_similarity_avg = 0.0\n",
    "    if len(comment_bodies) > 1:\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english').fit_transform(comment_bodies)\n",
    "            cosine_matrix = cosine_similarity(vectorizer)\n",
    "            upper_triangle_indices = np.triu_indices_from(cosine_matrix, k=1)\n",
    "            if upper_triangle_indices[0].size > 0:\n",
    "                comment_similarity_avg = np.mean(cosine_matrix[upper_triangle_indices])\n",
    "        except ValueError:\n",
    "            comment_similarity_avg = 0.0\n",
    "    exact_duplicate_ratio = 0.0\n",
    "    if len(comment_bodies) > 0:\n",
    "        num_unique_comments = len(set(comment_bodies))\n",
    "        exact_duplicate_ratio = (len(comment_bodies) - num_unique_comments) / len(comment_bodies)\n",
    "    features = {\n",
    "        'account_age_days': account_age_days, 'karma_ratio': karma_ratio,\n",
    "        'avg_time_between_posts_sec': avg_time_between_posts_sec, 'subreddit_entropy': subreddit_entropy,\n",
    "        'username_is_pattern': username_is_pattern, 'comment_length_variance': comment_length_variance,\n",
    "        'submission_comment_ratio': submission_comment_ratio, 'link_in_comment_ratio': link_in_comment_ratio,\n",
    "        'sentiment_avg': sentiment_avg, 'sentiment_variance': sentiment_variance,\n",
    "        'comment_similarity_avg': comment_similarity_avg, 'exact_duplicate_ratio': exact_duplicate_ratio,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def analyze_with_llm(username, reddit_instance, metrics_data, model_name=\"gemini-1.5-flash\"):\n",
    "    \"\"\"Usa um LLM (Gemini) para analisar os dados de um usu√°rio.\"\"\"\n",
    "    try:\n",
    "        user = reddit_instance.redditor(username)\n",
    "        comments = list(user.comments.new(limit=25))\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"N√£o foi poss√≠vel buscar os dados do usu√°rio: {e}\"}\n",
    "\n",
    "    if len(comments) < 5:\n",
    "        prompt = f\"\"\"\n",
    "        Voc√™ √© um analista especialista em detec√ß√£o de bots na plataforma Reddit. Sua tarefa √© fazer uma\n",
    "        an√°lise do usu√°rio u/{username} baseada APENAS nos seus dados quantitativos, pois\n",
    "        ele n√£o possui coment√°rios recentes suficientes para uma an√°lise de texto.\n",
    "\n",
    "        --- DADOS QUANTITATIVOS ---\n",
    "        * Idade da Conta (dias): {metrics_data.get('account_age_days', 'N/A')} (Contas muito novas s√£o suspeitas)\n",
    "        * Propor√ß√£o de Karma (coment√°rio/link): {metrics_data.get('karma_ratio', 0.0):.2f} (Valores extremos podem ser suspeitos)\n",
    "        * Taxa de Duplica√ß√£o Exata de Coment√°rios: {metrics_data.get('exact_duplicate_ratio', 0.0):.2f} (Uma taxa > 0.25 √© forte sinal de bot)\n",
    "        * Tempo M√©dio entre Atividades (segundos): {metrics_data.get('avg_time_between_posts_sec', 0.0):.2f} (Valores muito baixos podem indicar automa√ß√£o)\n",
    "        * Entropia de Subreddits: {metrics_data.get('subreddit_entropy', 0.0):.2f} (Valores muito baixos indicam atividade concentrada)\n",
    "        * Vari√¢ncia do Tamanho dos Coment√°rios: {metrics_data.get('comment_length_variance', 0.0):.2f} (Pr√≥ximo de zero indica coment√°rios de tamanho similar)\n",
    "        ---\n",
    "\n",
    "        Com base APENAS nos dados quantitativos acima, fa√ßa sua avalia√ß√£o final.\n",
    "\n",
    "        Responda estritamente no formato JSON a seguir, sem texto adicional:\n",
    "        {{\n",
    "          \"veredicto\": \"bot\" | \"humano\",\n",
    "          \"confianca\": <um n√∫mero de 0.0 a 1.0>,\n",
    "          \"justificativa\": \"<uma an√°lise concisa de 2-3 frases explicando sua decis√£o com base APENAS nos dados num√©ricos. Mencione a aus√™ncia de coment√°rios.>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        comment_list_for_prompt = \"\\\\n\".join([f\"{i+1}. {c.body.strip()}\" for i, c in enumerate(comments)])\n",
    "        prompt = f\"\"\"\n",
    "        Voc√™ √© um analista especialista em detec√ß√£o de bots na plataforma Reddit. Sua tarefa √© fazer uma\n",
    "        an√°lise h√≠brida, considerando tanto os dados quantitativos quanto o conte√∫do dos coment√°rios\n",
    "        recentes do usu√°rio u/{username}.\n",
    "\n",
    "        --- DADOS QUANTITATIVOS ---\\n\n",
    "        * Idade da Conta (dias): {metrics_data.get('account_age_days', 'N/A')} (Contas muito novas s√£o mais suspeitas)\\n\n",
    "        * Propor√ß√£o de Karma (coment√°rio/link): {metrics_data.get('karma_ratio', 0.0):.2f} (Valores extremos podem ser suspeitos)\\n\n",
    "        * Taxa de Duplica√ß√£o Exata de Coment√°rios: {metrics_data.get('exact_duplicate_ratio', 0.0):.2f} (Uma taxa acima de 0.25 √© um forte sinal de bot)\n",
    "\n",
    "        --- COMENT√ÅRIOS RECENTES ({len(comments)}) ---\\n\n",
    "        {comment_list_for_prompt}\\n\n",
    "        ---\n",
    "\n",
    "        Com base em TODOS os dados acima (quantitativos e textuais), fa√ßa sua avalia√ß√£o final.\n",
    "        Considere como os dados num√©ricos refor√ßam ou contradizem a an√°lise do texto.\n",
    "\n",
    "        Responda estritamente no formato JSON a seguir, sem texto adicional:\n",
    "        {{\n",
    "          \"veredicto\": \"bot\" | \"humano\",\n",
    "          \"confianca\": <um n√∫mero de 0.0 a 1.0>,\n",
    "          \"justificativa\": \"<uma an√°lise concisa de 2-3 frases explicando sua decis√£o com base nos dados e nos coment√°rios>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        cleaned_response = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        return json.loads(cleaned_response)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Erro ao analisar com o LLM: {e}\", \"raw_response\": response.text if 'response' in locals() else \"\"}\n",
    "\n",
    "\n",
    "# --- 3. GRADIO INTERFACE LOGIC ---\n",
    "# This cell contains the functions that will power the Gradio interface.\n",
    "\n",
    "def setup_apis(google_key, reddit_id, reddit_secret, reddit_agent):\n",
    "    \"\"\"Configura as APIs com as chaves fornecidas pela interface.\"\"\"\n",
    "    global reddit_instance, user_db, is_configured\n",
    "    \n",
    "    # Configurar Google AI\n",
    "    try:\n",
    "        genai.configure(api_key=google_key)\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erro na configura√ß√£o do Google AI: {e}\", gr.update(visible=False), gr.update(visible=False)\n",
    "\n",
    "    # Configurar Reddit PRAW\n",
    "    try:\n",
    "        reddit_instance = praw.Reddit(\n",
    "            client_id=reddit_id,\n",
    "            client_secret=reddit_secret,\n",
    "            user_agent=reddit_agent\n",
    "        )\n",
    "        if reddit_instance.read_only:\n",
    "            status_msg = \"‚úÖ APIs configuradas (Reddit em modo somente leitura).\"\n",
    "        else:\n",
    "            status_msg = f\"‚úÖ APIs configuradas (Reddit autenticado como u/{reddit_instance.user.me()}).\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erro na configura√ß√£o do Reddit PRAW: {e}\", gr.update(visible=False), gr.update(visible=False)\n",
    "\n",
    "    # Carregar banco de dados\n",
    "    user_db = load_database()\n",
    "    status_msg += f\"\\nüíæ Banco de dados carregado com {len(user_db)} entradas.\"\n",
    "    is_configured = True\n",
    "    \n",
    "    # Retorna a mensagem de status e mostra os pr√≥ximos elementos da UI\n",
    "    return status_msg, gr.update(visible=True), gr.update(visible=True)\n",
    "\n",
    "\n",
    "def run_full_analysis(url):\n",
    "    \"\"\"Fun√ß√£o principal que √© acionada pelo bot√£o 'Analisar'.\"\"\"\n",
    "    if not is_configured or not reddit_instance:\n",
    "        return \"Erro: As APIs n√£o foram configuradas. Por favor, preencha e salve suas credenciais primeiro.\", \"\", \"\", gr.update(visible=False), None\n",
    "\n",
    "    match = re.search(r\"/user/([A-Za-z0-9_-]+)\", url)\n",
    "    if not match:\n",
    "        return \"‚ùå URL inv√°lida. Use o formato: https://www.reddit.com/user/username/\", \"\", \"\", gr.update(visible=False), None\n",
    "    \n",
    "    username = match.group(1)\n",
    "    \n",
    "    # 1. An√°lise de M√©tricas\n",
    "    status_log = f\"üì° 1/2: Calculando m√©tricas para u/{username}...\"\n",
    "    yield status_log, \"\", \"\", gr.update(visible=False), None\n",
    "    \n",
    "    features = analyze_user_live(username, reddit_instance)\n",
    "    if \"error\" in features:\n",
    "        error_msg = f\"‚ùå Erro na an√°lise de m√©tricas: {features['error']}\"\n",
    "        yield error_msg, \"\", \"\", gr.update(visible=False), None\n",
    "        return\n",
    "\n",
    "    metrics_md = \"### üìà M√©tricas Quantitativas\\n---\\n\"\n",
    "    for key, value in features.items():\n",
    "        metrics_md += f\"- **{key.replace('_', ' ').title()}**: {value:.4f}\\n\"\n",
    "    \n",
    "    # 2. An√°lise com LLM\n",
    "    status_log += f\"\\nüß† 2/2: Solicitando an√°lise do Gemini para u/{username}...\"\n",
    "    yield status_log, metrics_md, \"\", gr.update(visible=False), None\n",
    "    \n",
    "    llm_analysis = analyze_with_llm(username, reddit_instance, features)\n",
    "    \n",
    "    if \"error\" in llm_analysis:\n",
    "        error_msg = f\"‚ùå Erro na an√°lise do LLM: {llm_analysis['error']}\"\n",
    "        ai_verdict_md = f\"### ü§ñ Veredito da IA\\n---\\n{error_msg}\"\n",
    "        status_log += \"\\n‚ùå An√°lise falhou.\"\n",
    "        yield status_log, metrics_md, ai_verdict_md, gr.update(visible=False), None\n",
    "        return\n",
    "\n",
    "    verdict = llm_analysis.get('veredicto', 'N/A').upper()\n",
    "    confidence = llm_analysis.get('confianca', 0.0)\n",
    "    justification = llm_analysis.get('justificativa', 'N/A')\n",
    "\n",
    "    emoji = \"ü§ñ\" if verdict == \"BOT\" else \"üßë\"\n",
    "    ai_verdict_md = f\"### {emoji} Veredito da IA\\n---\\n\"\n",
    "    ai_verdict_md += f\"**Veredito:** `{verdict}`\\n\\n\"\n",
    "    ai_verdict_md += f\"**Confian√ßa:** `{confidence:.2%}`\\n\\n\"\n",
    "    ai_verdict_md += f\"**Justificativa:** *{justification}*\"\n",
    "\n",
    "    status_log += f\"\\n‚úÖ An√°lise conclu√≠da! Veredito: {verdict}. Por favor, salve seu veredito final.\"\n",
    "\n",
    "    # Retorna todos os resultados e mostra os bot√µes de salvar\n",
    "    yield status_log, metrics_md, ai_verdict_md, gr.update(visible=True), features\n",
    "\n",
    "\n",
    "def save_final_verdict(features, label):\n",
    "    \"\"\"Salva a entrada no banco de dados com o r√≥tulo final do usu√°rio.\"\"\"\n",
    "    global user_db\n",
    "    if features is None:\n",
    "        return \"Nada para salvar.\", f\"üíæ Banco de dados: {len(user_db)} entradas.\"\n",
    "\n",
    "    is_bot_label = 1 if label == 'bot' else 0\n",
    "    features['is_bot'] = is_bot_label\n",
    "    \n",
    "    new_entry = pd.DataFrame([features])\n",
    "    user_db = pd.concat([user_db, new_entry], ignore_index=True)\n",
    "    user_db.to_csv(DATABASE_FILE, index=False)\n",
    "    \n",
    "    confirmation_msg = f\"‚úÖ Veredito '{label.upper()}' salvo! O banco de dados agora tem {len(user_db)} entradas.\"\n",
    "    db_status_msg = f\"üíæ Banco de dados: {len(user_db)} entradas. Salvo em '{DATABASE_FILE}'.\"\n",
    "    \n",
    "    return confirmation_msg, db_status_msg\n",
    "\n",
    "# --- 4. GRADIO UI DEFINITION ---\n",
    "# Esta c√©lula constr√≥i e executa a interface web.\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Reddit Bot Detector\") as demo:\n",
    "    gr.Markdown(\"# ü§ñ Reddit Bot Detector com IA H√≠brida\")\n",
    "    gr.Markdown(\"Uma ferramenta para analisar perfis de usu√°rios do Reddit usando m√©tricas quantitativas e an√°lise da IA do Google (Gemini).\")\n",
    "\n",
    "    # Estado para armazenar os 'features' entre a an√°lise e o salvamento\n",
    "    analysis_features = gr.State(value=None)\n",
    "    \n",
    "    with gr.Accordion(\"üîë Configura√ß√£o de APIs (Obrigat√≥rio)\", open=True) as setup_accordion:\n",
    "        with gr.Row():\n",
    "            google_key_input = gr.Textbox(label=\"Google AI API Key\", type=\"password\")\n",
    "        with gr.Row():\n",
    "            reddit_id_input = gr.Textbox(label=\"Reddit Client ID\", type=\"password\")\n",
    "            reddit_secret_input = gr.Textbox(label=\"Reddit Client Secret\", type=\"password\")\n",
    "            reddit_agent_input = gr.Textbox(label=\"Reddit User Agent\", value=\"SubredditAnalyzer/1.0 by YourUsername\")\n",
    "        setup_button = gr.Button(\"Salvar Credenciais e Iniciar\")\n",
    "    \n",
    "    status_log_box = gr.Textbox(label=\"Status\", interactive=False, lines=2)\n",
    "    \n",
    "    # ‚ùóÔ∏è CORRE√á√ÉO: Trocado gr.Box por gr.Group\n",
    "    with gr.Group(visible=False) as analysis_box:\n",
    "        with gr.Row():\n",
    "            url_input = gr.Textbox(label=\"URL do Perfil do Usu√°rio Reddit\", placeholder=\"https://www.reddit.com/user/SomeUser\")\n",
    "            analyze_button = gr.Button(\"üîç Analisar\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            metrics_output = gr.Markdown(label=\"M√©tricas Quantitativas\")\n",
    "            ai_verdict_output = gr.Markdown(label=\"Veredito da IA\")\n",
    "    \n",
    "    # ‚ùóÔ∏è CORRE√á√ÉO: Trocado gr.Box por gr.Group\n",
    "    with gr.Group(visible=False) as verdict_box:\n",
    "        gr.Markdown(\"--- \\n### üëâ Qual √© o seu veredito final?\")\n",
    "        gr.Markdown(\"Com base nos dados acima, classifique o usu√°rio. Sua resposta treinar√° o modelo futuro.\")\n",
    "        with gr.Row():\n",
    "            save_bot_button = gr.Button(\"CONFIRMAR COMO BOT ü§ñ\")\n",
    "            save_human_button = gr.Button(\"CONFIRMAR COMO HUMANO üßë\")\n",
    "        db_status_box = gr.Textbox(label=\"Status do Banco de Dados\", interactive=False)\n",
    "\n",
    "    # L√≥gica dos Eventos da UI\n",
    "    setup_button.click(\n",
    "        fn=setup_apis,\n",
    "        inputs=[google_key_input, reddit_id_input, reddit_secret_input, reddit_agent_input],\n",
    "        outputs=[status_log_box, analysis_box, verdict_box]\n",
    "    )\n",
    "    \n",
    "    analyze_button.click(\n",
    "        fn=run_full_analysis,\n",
    "        inputs=[url_input],\n",
    "        outputs=[status_log_box, metrics_output, ai_verdict_output, verdict_box, analysis_features]\n",
    "    )\n",
    "    \n",
    "    save_bot_button.click(\n",
    "        fn=lambda features: save_final_verdict(features, 'bot'),\n",
    "        inputs=[analysis_features],\n",
    "        outputs=[status_log_box, db_status_box]\n",
    "    )\n",
    "    \n",
    "    save_human_button.click(\n",
    "        fn=lambda features: save_final_verdict(features, 'human'),\n",
    "        inputs=[analysis_features],\n",
    "        outputs=[status_log_box, db_status_box]\n",
    "    )\n",
    "\n",
    "# Inicia a interface\n",
    "demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
