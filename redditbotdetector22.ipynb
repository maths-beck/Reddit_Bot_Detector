{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac54baae-01c5-4540-a290-ac2937ffc689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API do Google AI configurada com sucesso.\n",
      "✅ Todas as bibliotecas foram importadas e a configuração foi concluída.\n"
     ]
    }
   ],
   "source": [
    "# Instala todas as bibliotecas necessárias, incluindo a do Google AI\n",
    "!pip install praw pandas numpy scipy scikit-learn vadersentiment nltk google-generativeai -q\n",
    "\n",
    "# Importações gerais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "import praw\n",
    "import json\n",
    "\n",
    "# Importações para Análise de Texto (PLN)\n",
    "import nltk\n",
    "from scipy.stats import entropy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Importações do Google AI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Baixa diretamente os pacotes de dados necessários para o NLTK.\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# --- FUNÇÃO DE CONFIGURAÇÃO DA GOOGLE AI API KEY ---\n",
    "# Esta função recebe a chave da API como um parâmetro e configura a biblioteca.\n",
    "def configure_google_ai(api_key):\n",
    "    \"\"\"\n",
    "    Configura a API do Google AI com a chave fornecida.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Sua chave de API do Google AI.\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        print(\"⚠️ Chave de API não fornecida. A funcionalidade do Google AI estará desativada.\")\n",
    "        return\n",
    "    try:\n",
    "        genai.configure(api_key=api_key)\n",
    "        print(\"✅ API do Google AI configurada com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ocorreu um erro ao configurar a API do Google AI: {e}\")\n",
    "\n",
    "# --- COMO USAR ---\n",
    "# 1. Substitua \"SUA_CHAVE_DE_API_AQUI\" pela sua chave de API real.\n",
    "# 2. Execute a célula.\n",
    "\n",
    "# Exemplo de uso:\n",
    "# Coloque sua chave de API diretamente na variável abaixo.\n",
    "YOUR_GOOGLE_API_KEY = \"AIzaSyDV6VITxV5WwCgoKAh3nSlXhk1r4ZWZhSU\" \n",
    "\n",
    "# Chama a função para configurar a API\n",
    "configure_google_ai(YOUR_GOOGLE_API_KEY)\n",
    "\n",
    "print(\"✅ Todas as bibliotecas foram importadas e a configuração foi concluída.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc40f42-459f-49ea-8483-aae8a6273e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replace these placeholders with your credentials ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"a9IG6BCyE9K5Il_1fbqszA\",\n",
    "        client_secret=\"NZtv0dtUDP6mLZJJ8xQteyFvzfBdvw\",\n",
    "    user_agent=\"SubredditAnalyzer/1.0 by u/YourUsername\"\n",
    ")\n",
    "\n",
    "# Check if the authentication is read-only or successful\n",
    "if reddit.read_only:\n",
    "    print(\"⚠  Warning: Authenticated in read-only mode. Check your credentials.\")\n",
    "else:\n",
    "    print(f\"✅ Authenticated successfully as u/{reddit.user.me()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c922610-b9db-4522-8d37-286f492d6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = 'user_database.csv'\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Carrega o banco de dados de características do usuário ou o cria.\"\"\"\n",
    "    try:\n",
    "        db = pd.read_csv(DATABASE_FILE)\n",
    "    except FileNotFoundError:\n",
    "        db = pd.DataFrame(columns=[\n",
    "            'account_age_days', 'karma_ratio', 'avg_time_between_posts_sec',\n",
    "            'subreddit_entropy', 'username_is_pattern', 'comment_length_variance',\n",
    "            'submission_comment_ratio', 'link_in_comment_ratio',\n",
    "            'sentiment_avg', 'sentiment_variance', 'comment_similarity_avg',\n",
    "            'exact_duplicate_ratio', 'is_bot'\n",
    "        ])\n",
    "    return db\n",
    "\n",
    "def analyze_user_live(username, reddit_instance):\n",
    "    \"\"\"Analisa um usuário com um conjunto completo de métricas numéricas.\"\"\"\n",
    "    # ... esta função permanece exatamente a mesma da versão anterior ...\n",
    "    try:\n",
    "        user = reddit_instance.redditor(username)\n",
    "        _ = user.id\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Não foi possível encontrar ou acessar o usuário '{username}'. Erro: {e}\"}\n",
    "    comments = list(user.comments.new(limit=100))\n",
    "    submissions = list(user.submissions.new(limit=50))\n",
    "    activities = sorted(comments + submissions, key=lambda x: x.created_utc, reverse=True)\n",
    "    if len(comments) < 5:\n",
    "        # Este retorno de erro é para a análise de métricas; a análise do LLM tem sua própria lógica.\n",
    "        # Mantemos isso para garantir que as métricas sejam baseadas em dados suficientes.\n",
    "        # No entanto, se o objetivo é analisar mesmo sem comentários, podemos ajustar aqui também.\n",
    "        # Por enquanto, a lógica principal tratará um usuário sem comentários.\n",
    "        pass # Permite a continuação para que o LLM possa analisar apenas as métricas de perfil.\n",
    "\n",
    "    created_date = datetime.fromtimestamp(user.created_utc)\n",
    "    account_age_days = (datetime.now() - created_date).days\n",
    "    karma_ratio = user.comment_karma / (user.link_karma + 1) if user.link_karma > 0 else user.comment_karma\n",
    "    timestamps = [act.created_utc for act in activities]\n",
    "    time_deltas = np.diff(timestamps)\n",
    "    avg_time_between_posts_sec = -np.mean(time_deltas) if len(time_deltas) > 0 else 0\n",
    "    subreddits = [act.subreddit.display_name for act in activities if hasattr(act, 'subreddit')]\n",
    "    counts = pd.Series(subreddits).value_counts()\n",
    "    subreddit_entropy = entropy(counts) if not counts.empty else 0\n",
    "    pattern = r\"[a-zA-Z]+[-_][a-zA-Z]+[0-9]{2,}\"\n",
    "    username_is_pattern = 1 if re.search(pattern, username) else 0\n",
    "    comment_lengths = [len(c.body) for c in comments]\n",
    "    comment_length_variance = np.var(comment_lengths) if comment_lengths else 0\n",
    "    submission_comment_ratio = len(submissions) / (len(comments) + 1)\n",
    "    link_count = sum(1 for c in comments if 'http' in c.body.lower())\n",
    "    link_in_comment_ratio = link_count / len(comments) if comments else 0\n",
    "    comment_bodies = [c.body for c in comments]\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = [analyzer.polarity_scores(comment)['compound'] for comment in comment_bodies]\n",
    "    sentiment_avg = np.mean(sentiment_scores) if sentiment_scores else 0\n",
    "    sentiment_variance = np.var(sentiment_scores) if sentiment_scores else 0\n",
    "    comment_similarity_avg = 0.0\n",
    "    if len(comment_bodies) > 1:\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english').fit_transform(comment_bodies)\n",
    "            cosine_matrix = cosine_similarity(vectorizer)\n",
    "            upper_triangle_indices = np.triu_indices_from(cosine_matrix, k=1)\n",
    "            if upper_triangle_indices[0].size > 0:\n",
    "                comment_similarity_avg = np.mean(cosine_matrix[upper_triangle_indices])\n",
    "        except ValueError:\n",
    "            comment_similarity_avg = 0.0\n",
    "    exact_duplicate_ratio = 0.0\n",
    "    if len(comment_bodies) > 0:\n",
    "        num_unique_comments = len(set(comment_bodies))\n",
    "        exact_duplicate_ratio = (len(comment_bodies) - num_unique_comments) / len(comment_bodies)\n",
    "    features = {\n",
    "        'account_age_days': account_age_days, 'karma_ratio': karma_ratio,\n",
    "        'avg_time_between_posts_sec': avg_time_between_posts_sec, 'subreddit_entropy': subreddit_entropy,\n",
    "        'username_is_pattern': username_is_pattern, 'comment_length_variance': comment_length_variance,\n",
    "        'submission_comment_ratio': submission_comment_ratio, 'link_in_comment_ratio': link_in_comment_ratio,\n",
    "        'sentiment_avg': sentiment_avg, 'sentiment_variance': sentiment_variance,\n",
    "        'comment_similarity_avg': comment_similarity_avg, 'exact_duplicate_ratio': exact_duplicate_ratio,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "# --- FUNÇÃO ATUALIZADA ---\n",
    "def analyze_with_llm(username, reddit_instance, metrics_data, model_name=\"gemini-1.5-flash\"):\n",
    "    \"\"\"\n",
    "    Usa um LLM (Gemini) para analisar os dados de um usuário.\n",
    "    Se o usuário tiver comentários, a análise é híbrida (métricas + texto).\n",
    "    Se não tiver comentários, a análise usa apenas as métricas numéricas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user = reddit_instance.redditor(username)\n",
    "        # Tenta buscar os comentários, mas não gera erro imediato se não houver\n",
    "        comments = list(user.comments.new(limit=25))\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Não foi possível buscar os dados do usuário: {e}\"}\n",
    "\n",
    "    # --- LÓGICA MODIFICADA ---\n",
    "    # Decide qual prompt usar com base na existência de comentários.\n",
    "\n",
    "    if len(comments) < 5:\n",
    "        # --- PROMPT 1: APENAS MÉTRICAS (QUANDO NÃO HÁ COMENTÁRIOS) ---\n",
    "        # Adicionei mais algumas métricas para dar mais contexto ao LLM\n",
    "        prompt = f\"\"\"\n",
    "        Você é um analista especialista em detecção de bots na plataforma Reddit. Sua tarefa é fazer uma\n",
    "        análise do usuário u/{username} baseada APENAS nos seus dados quantitativos, pois\n",
    "        ele não possui comentários recentes suficientes para uma análise de texto.\n",
    "\n",
    "        --- DADOS QUANTITATIVOS ---\n",
    "        * Idade da Conta (dias): {metrics_data.get('account_age_days', 'N/A')} (Contas muito novas são suspeitas)\n",
    "        * Proporção de Karma (comentário/link): {metrics_data.get('karma_ratio', 0.0):.2f} (Valores extremos podem ser suspeitos)\n",
    "        * Taxa de Duplicação Exata de Comentários: {metrics_data.get('exact_duplicate_ratio', 0.0):.2f} (Uma taxa > 0.25 é forte sinal de bot)\n",
    "        * Tempo Médio entre Atividades (segundos): {metrics_data.get('avg_time_between_posts_sec', 0.0):.2f} (Valores muito baixos podem indicar automação)\n",
    "        * Entropia de Subreddits: {metrics_data.get('subreddit_entropy', 0.0):.2f} (Valores muito baixos indicam atividade concentrada)\n",
    "        * Variância do Tamanho dos Comentários: {metrics_data.get('comment_length_variance', 0.0):.2f} (Próximo de zero indica comentários de tamanho similar)\n",
    "        ---\n",
    "\n",
    "        Com base APENAS nos dados quantitativos acima, faça sua avaliação final.\n",
    "\n",
    "        Responda estritamente no formato JSON a seguir, sem texto adicional:\n",
    "        {{\n",
    "          \"veredicto\": \"bot\" | \"humano\",\n",
    "          \"confianca\": <um número de 0.0 a 1.0>,\n",
    "          \"justificativa\": \"<uma análise concisa de 2-3 frases explicando sua decisão com base APENAS nos dados numéricos. Mencione a ausência de comentários.>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # --- PROMPT 2: ANÁLISE HÍBRIDA (O SEU PROMPT ORIGINAL) ---\n",
    "        comment_list_for_prompt = \"\\\\n\".join([f\"{i+1}. {c.body.strip()}\" for i, c in enumerate(comments)])\n",
    "        prompt = f\"\"\"\n",
    "        Você é um analista especialista em detecção de bots na plataforma Reddit. Sua tarefa é fazer uma\n",
    "        análise híbrida, considerando tanto os dados quantitativos quanto o conteúdo dos comentários\n",
    "        recentes do usuário u/{username}.\n",
    "\n",
    "        --- DADOS QUANTITATIVOS ---\\n\n",
    "        * Idade da Conta (dias): {metrics_data.get('account_age_days', 'N/A')} (Contas muito novas são mais suspeitas)\\n\n",
    "        * Proporção de Karma (comentário/link): {metrics_data.get('karma_ratio', 0.0):.2f} (Valores extremos podem ser suspeitos)\\n\n",
    "        * Taxa de Duplicação Exata de Comentários: {metrics_data.get('exact_duplicate_ratio', 0.0):.2f} (Uma taxa acima de 0.25 é um forte sinal de bot)\n",
    "\n",
    "        --- COMENTÁRIOS RECENTES ({len(comments)}) ---\\n\n",
    "        {comment_list_for_prompt}\\n\n",
    "        ---\n",
    "\n",
    "        Com base em TODOS os dados acima (quantitativos e textuais), faça sua avaliação final.\n",
    "        Considere como os dados numéricos reforçam ou contradizem a análise do texto.\n",
    "\n",
    "        Responda estritamente no formato JSON a seguir, sem texto adicional:\n",
    "        {{\n",
    "          \"veredicto\": \"bot\" | \"humano\",\n",
    "          \"confianca\": <um número de 0.0 a 1.0>,\n",
    "          \"justificativa\": \"<uma análise concisa de 2-3 frases explicando sua decisão com base nos dados e nos comentários>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "    # O restante da função permanece igual\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        cleaned_response = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        return json.loads(cleaned_response)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Erro ao analisar com o LLM: {e}\", \"raw_response\": response.text if 'response' in locals() else \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1997129-453d-44f7-a6d0-aacf2264aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Banco de dados carregado com 26 entradas.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Digite a URL do perfil do usuário para analisar (ou 'quit' para sair):  https://www.reddit.com/user/twitch_and_shock/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📡 1/2: Calculando métricas numéricas para u/twitch_and_shock...\n",
      "\n",
      "--- Resultados da Análise de Métricas ---\n",
      "  - account_age_days: 1522.0000\n",
      "  - karma_ratio: 2.6310\n",
      "  - avg_time_between_posts_sec: 775919.1132\n",
      "  - subreddit_entropy: 2.3666\n",
      "  - username_is_pattern: 0.0000\n",
      "  - comment_length_variance: 83629.1379\n",
      "  - submission_comment_ratio: 0.0693\n",
      "  - link_in_comment_ratio: 0.0300\n",
      "  - sentiment_avg: 0.2016\n",
      "  - sentiment_variance: 0.1817\n",
      "  - comment_similarity_avg: 0.0152\n",
      "  - exact_duplicate_ratio: 0.0000\n",
      "\n",
      "🧠 2/2: Solicitando análise do LLM para u/twitch_and_shock...\n",
      "\n",
      "--- Veredito do LLM (Análise Híbrida) ---\n",
      "  - Veredito: HUMANO\n",
      "  - Confiança: 0.80\n",
      "  - Justificativa: Apesar da idade da conta ser relativamente alta e a proporção de karma estar acima da média, a ausência de duplicação de comentários e o conteúdo consistente e técnico dos comentários sugerem um usuário humano.  Os comentários demonstram conhecimento específico de programação e ferramentas de desenvolvimento, inconsistente com um bot padrão.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Com base em TUDO, qual é o seu veredito final: bot (b) ou humano (h)?  h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Entrada adicionada! O banco de dados agora tem 27 entradas.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Digite a URL do perfil do usuário para analisar (ou 'quit' para sair):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Banco de dados salvo com sucesso em 'user_database.csv'.\n"
     ]
    }
   ],
   "source": [
    "user_db = load_database()\n",
    "print(f\"💾 Banco de dados carregado com {len(user_db)} entradas.\")\n",
    "\n",
    "while True:\n",
    "    url = input(\"Digite a URL do perfil do usuário para analisar (ou 'quit' para sair): \")\n",
    "    if url.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    match = re.search(r\"/user/([A-Za-z0-9_-]+)\", url)\n",
    "    if not match:\n",
    "        print(\"❌ Formato de URL inválido. Use o formato: https://www.reddit.com/user/username/\")\n",
    "        continue\n",
    "    username = match.group(1)\n",
    "\n",
    "    # --- Etapa 1: Análise de Métricas ---\n",
    "    print(f\"\\n📡 1/2: Calculando métricas numéricas para u/{username}...\")\n",
    "    features = analyze_user_live(username, reddit)\n",
    "    if \"error\" in features:\n",
    "        print(f\"❌ Erro na análise de métricas: {features['error']}\")\n",
    "        continue\n",
    "    print(\"\\n--- Resultados da Análise de Métricas ---\")\n",
    "    for key, value in features.items():\n",
    "        print(f\"  - {key}: {value:.4f}\")\n",
    "\n",
    "    # --- Etapa 2: Análise com LLM (usando as métricas da Etapa 1) ---\n",
    "    print(f\"\\n🧠 2/2: Solicitando análise do LLM para u/{username}...\")\n",
    "    # Passa o dicionário 'features' para a função do LLM\n",
    "    llm_analysis = analyze_with_llm(username, reddit, features)\n",
    "    print(\"\\n--- Veredito do LLM (Análise Híbrida) ---\")\n",
    "    if \"error\" in llm_analysis:\n",
    "        print(f\"❌ Erro na análise do LLM: {llm_analysis['error']}\")\n",
    "        if 'raw_response' in llm_analysis:\n",
    "            print(f\"   Resposta Bruta: {llm_analysis['raw_response']}\")\n",
    "    else:\n",
    "        print(f\"  - Veredito: {llm_analysis.get('veredicto', 'N/A').upper()}\")\n",
    "        print(f\"  - Confiança: {llm_analysis.get('confianca', 0.0):.2f}\")\n",
    "        print(f\"  - Justificativa: {llm_analysis.get('justificativa', 'N/A')}\")\n",
    "\n",
    "    # --- Etapa 3: Coleta do seu rótulo final ---\n",
    "    label = ''\n",
    "    while label not in ['b', 'h']:\n",
    "        label = input(\"\\n--> Com base em TUDO, qual é o seu veredito final: bot (b) ou humano (h)? \").lower()\n",
    "\n",
    "    features['is_bot'] = 1 if label == 'b' else 0\n",
    "    new_entry = pd.DataFrame([features])\n",
    "    user_db = pd.concat([user_db, new_entry], ignore_index=True)\n",
    "    print(f\"✅ Entrada adicionada! O banco de dados agora tem {len(user_db)} entradas.\\n\")\n",
    "\n",
    "# Salva o banco de dados final no arquivo CSV\n",
    "user_db.to_csv(DATABASE_FILE, index=False)\n",
    "print(f\"\\n💾 Banco de dados salvo com sucesso em '{DATABASE_FILE}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81646bc-0ed6-4361-a142-14eb283aae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Visualização do Banco de Dados (26 entradas) ---\n",
      "\n",
      "--- Primeiras 5 Entradas ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_age_days</th>\n",
       "      <th>karma_ratio</th>\n",
       "      <th>avg_time_between_posts_sec</th>\n",
       "      <th>subreddit_entropy</th>\n",
       "      <th>username_is_pattern</th>\n",
       "      <th>comment_length_variance</th>\n",
       "      <th>submission_comment_ratio</th>\n",
       "      <th>link_in_comment_ratio</th>\n",
       "      <th>is_bot</th>\n",
       "      <th>sentiment_avg</th>\n",
       "      <th>sentiment_variance</th>\n",
       "      <th>comment_similarity_avg</th>\n",
       "      <th>exact_duplicate_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>2.759855e+04</td>\n",
       "      <td>1.860609</td>\n",
       "      <td>0</td>\n",
       "      <td>4.651600</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166</td>\n",
       "      <td>0.045242</td>\n",
       "      <td>1.394641e+04</td>\n",
       "      <td>2.997718</td>\n",
       "      <td>0</td>\n",
       "      <td>2549.250000</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1850</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>2.147339e+06</td>\n",
       "      <td>2.557894</td>\n",
       "      <td>1</td>\n",
       "      <td>3154.016529</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>405</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>3.007383e+05</td>\n",
       "      <td>1.578693</td>\n",
       "      <td>0</td>\n",
       "      <td>796.372449</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>405</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>3.007383e+05</td>\n",
       "      <td>1.578693</td>\n",
       "      <td>0</td>\n",
       "      <td>796.372449</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266357</td>\n",
       "      <td>0.239542</td>\n",
       "      <td>0.035965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_age_days  karma_ratio  avg_time_between_posts_sec  \\\n",
       "0               188     0.002024                2.759855e+04   \n",
       "1               166     0.045242                1.394641e+04   \n",
       "2              1850     0.291667                2.147339e+06   \n",
       "3               405     0.004955                3.007383e+05   \n",
       "4               405     0.004955                3.007383e+05   \n",
       "\n",
       "   subreddit_entropy  username_is_pattern  comment_length_variance  \\\n",
       "0           1.860609                    0                 4.651600   \n",
       "1           2.997718                    0              2549.250000   \n",
       "2           2.557894                    1              3154.016529   \n",
       "3           1.578693                    0               796.372449   \n",
       "4           1.578693                    0               796.372449   \n",
       "\n",
       "   submission_comment_ratio  link_in_comment_ratio  is_bot  sentiment_avg  \\\n",
       "0                  0.980392               0.000000       1            NaN   \n",
       "1                  0.980392               0.060000       1            NaN   \n",
       "2                  0.173913               0.000000       0            NaN   \n",
       "3                  3.333333               0.142857       0            NaN   \n",
       "4                  3.333333               0.142857       0       0.266357   \n",
       "\n",
       "   sentiment_variance  comment_similarity_avg  exact_duplicate_ratio  \n",
       "0                 NaN                     NaN                    NaN  \n",
       "1                 NaN                     NaN                    NaN  \n",
       "2                 NaN                     NaN                    NaN  \n",
       "3                 NaN                     NaN                    NaN  \n",
       "4            0.239542                0.035965                    NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Últimas 5 Entradas ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_age_days</th>\n",
       "      <th>karma_ratio</th>\n",
       "      <th>avg_time_between_posts_sec</th>\n",
       "      <th>subreddit_entropy</th>\n",
       "      <th>username_is_pattern</th>\n",
       "      <th>comment_length_variance</th>\n",
       "      <th>submission_comment_ratio</th>\n",
       "      <th>link_in_comment_ratio</th>\n",
       "      <th>is_bot</th>\n",
       "      <th>sentiment_avg</th>\n",
       "      <th>sentiment_variance</th>\n",
       "      <th>comment_similarity_avg</th>\n",
       "      <th>exact_duplicate_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>188</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>46695.429530</td>\n",
       "      <td>2.066065</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5075</td>\n",
       "      <td>0.49505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1833</td>\n",
       "      <td>42.130999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>166</td>\n",
       "      <td>0.044640</td>\n",
       "      <td>11772.966443</td>\n",
       "      <td>2.838560</td>\n",
       "      <td>0</td>\n",
       "      <td>3952.4224</td>\n",
       "      <td>0.49505</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.334498</td>\n",
       "      <td>0.139122</td>\n",
       "      <td>0.016795</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>188</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>46695.429530</td>\n",
       "      <td>2.066065</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5075</td>\n",
       "      <td>0.49505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>188</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>46695.429530</td>\n",
       "      <td>2.066065</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5075</td>\n",
       "      <td>0.49505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    account_age_days  karma_ratio  avg_time_between_posts_sec  \\\n",
       "21               188     0.002024                46695.429530   \n",
       "22              1833    42.130999                    0.000000   \n",
       "23               166     0.044640                11772.966443   \n",
       "24               188     0.002024                46695.429530   \n",
       "25               188     0.002024                46695.429530   \n",
       "\n",
       "    subreddit_entropy  username_is_pattern  comment_length_variance  \\\n",
       "21           2.066065                    0                   3.5075   \n",
       "22           0.000000                    0                   0.0000   \n",
       "23           2.838560                    0                3952.4224   \n",
       "24           2.066065                    0                   3.5075   \n",
       "25           2.066065                    0                   3.5075   \n",
       "\n",
       "    submission_comment_ratio  link_in_comment_ratio  is_bot  sentiment_avg  \\\n",
       "21                   0.49505                   0.00       1       0.000000   \n",
       "22                   0.00000                   0.00       1       0.000000   \n",
       "23                   0.49505                   0.14       1       0.334498   \n",
       "24                   0.49505                   0.00       1       0.000000   \n",
       "25                   0.49505                   0.00       1       0.000000   \n",
       "\n",
       "    sentiment_variance  comment_similarity_avg  exact_duplicate_ratio  \n",
       "21            0.000000                0.300000                   0.92  \n",
       "22            0.000000                0.000000                   0.00  \n",
       "23            0.139122                0.016795                   0.17  \n",
       "24            0.000000                0.300000                   0.92  \n",
       "25            0.000000                0.300000                   0.92  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    my_database = pd.read_csv(DATABASE_FILE)\n",
    "    print(f\"--- Visualização do Banco de Dados ({len(my_database)} entradas) ---\")\n",
    "    \n",
    "    # Exibe o DataFrame inteiro se não for muito grande, ou apenas o início e o fim.\n",
    "    if len(my_database) < 20:\n",
    "        display(my_database)\n",
    "    else:\n",
    "        print(\"\\n--- Primeiras 5 Entradas ---\")\n",
    "        display(my_database.head())\n",
    "        print(\"\\n--- Últimas 5 Entradas ---\")\n",
    "        display(my_database.tail())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"O arquivo de banco de dados '{DATABASE_FILE}' ainda não foi criado.\")\n",
    "    print(\"Execute a célula de coleta de dados pelo menos uma vez para criá-lo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470efeff-ff38-4003-873c-be49840db63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ffac1-6a08-4e63-881e-a8997210a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [21 lines of output]\n",
      "  + C:\\Users\\rrdki\\Documents\\Anaconda\\doc\\python.exe C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\vendored-meson\\meson\\meson.py setup C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\.mesonpy-md7m7zul -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\.mesonpy-md7m7zul\\meson-python-native-file.ini\n",
      "  The Meson build system\n",
      "  Version: 1.2.99\n",
      "  Source dir: C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\n",
      "  Build dir: C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\.mesonpy-md7m7zul\n",
      "  Build type: native build\n",
      "  Project name: NumPy\n",
      "  Project version: 1.26.4\n",
      "  WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "  \n",
      "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "  The following exception(s) were encountered:\n",
      "  Running `icl \"\"` gave \"[WinError 2] O sistema não pode encontrar o arquivo especificado\"\n",
      "  Running `cl /?` gave \"[WinError 2] O sistema não pode encontrar o arquivo especificado\"\n",
      "  Running `cc --version` gave \"[WinError 2] O sistema não pode encontrar o arquivo especificado\"\n",
      "  Running `gcc --version` gave \"[WinError 2] O sistema não pode encontrar o arquivo especificado\"\n",
      "  Running `clang --version` gave \"[WinError 2] O sistema não pode encontrar o arquivo especificado\"\n",
      "  Running `clang-cl /?` gave \"[WinError 2] O sistema não pode encontrar o arquivo especificado\"\n",
      "  Running `pgcc --version` gave \"[WinError 2] O sistema não pode encontrar o arquivo especificado\"\n",
      "  \n",
      "  A full log can be found at C:\\Users\\rrdki\\AppData\\Local\\Temp\\pip-install-19r5zf5u\\numpy_2efa80043511440baf3844482a3b211a\\.mesonpy-md7m7zul\\meson-logs\\meson-log.txt\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://ce689b1746565ccbfe.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ce689b1746565ccbfe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. SETUP: INSTALL LIBRARIES ---\n",
    "# This cell installs all necessary packages, including Gradio for the UI.\n",
    "!pip install praw pandas numpy scipy scikit-learn vadersentiment nltk google-generativeai \"gradio<4.0\" -q\n",
    "\n",
    "# --- 2. IMPORTS AND CORE LOGIC ---\n",
    "# This cell contains all your original backend functions without modification.\n",
    "\n",
    "# Importações gerais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "import praw\n",
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "# Importações para Análise de Texto (PLN)\n",
    "import nltk\n",
    "from scipy.stats import entropy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Importações do Google AI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Baixa pacotes de dados do NLTK.\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Variáveis globais para instâncias e banco de dados\n",
    "reddit_instance = None\n",
    "user_db = None\n",
    "DATABASE_FILE = 'user_database.csv'\n",
    "is_configured = False\n",
    "\n",
    "# --- Funções de Análise (Seu código original) ---\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Carrega o banco de dados de características do usuário ou o cria.\"\"\"\n",
    "    try:\n",
    "        db = pd.read_csv(DATABASE_FILE)\n",
    "    except FileNotFoundError:\n",
    "        db = pd.DataFrame(columns=[\n",
    "            'account_age_days', 'karma_ratio', 'avg_time_between_posts_sec',\n",
    "            'subreddit_entropy', 'username_is_pattern', 'comment_length_variance',\n",
    "            'submission_comment_ratio', 'link_in_comment_ratio',\n",
    "            'sentiment_avg', 'sentiment_variance', 'comment_similarity_avg',\n",
    "            'exact_duplicate_ratio', 'is_bot'\n",
    "        ])\n",
    "    return db\n",
    "\n",
    "def analyze_user_live(username, reddit_instance):\n",
    "    \"\"\"Analisa um usuário com um conjunto completo de métricas numéricas.\"\"\"\n",
    "    try:\n",
    "        user = reddit_instance.redditor(username)\n",
    "        _ = user.id\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Não foi possível encontrar ou acessar o usuário '{username}'. Erro: {e}\"}\n",
    "    comments = list(user.comments.new(limit=100))\n",
    "    submissions = list(user.submissions.new(limit=50))\n",
    "    activities = sorted(comments + submissions, key=lambda x: x.created_utc, reverse=True)\n",
    "    \n",
    "    created_date = datetime.fromtimestamp(user.created_utc)\n",
    "    account_age_days = (datetime.now() - created_date).days\n",
    "    karma_ratio = user.comment_karma / (user.link_karma + 1) if user.link_karma > 0 else user.comment_karma\n",
    "    timestamps = [act.created_utc for act in activities]\n",
    "    time_deltas = np.diff(timestamps)\n",
    "    avg_time_between_posts_sec = -np.mean(time_deltas) if len(time_deltas) > 0 else 0\n",
    "    subreddits = [act.subreddit.display_name for act in activities if hasattr(act, 'subreddit')]\n",
    "    counts = pd.Series(subreddits).value_counts()\n",
    "    subreddit_entropy = entropy(counts) if not counts.empty else 0\n",
    "    pattern = r\"[a-zA-Z]+[-_][a-zA-Z]+[0-9]{2,}\"\n",
    "    username_is_pattern = 1 if re.search(pattern, username) else 0\n",
    "    comment_lengths = [len(c.body) for c in comments]\n",
    "    comment_length_variance = np.var(comment_lengths) if comment_lengths else 0\n",
    "    submission_comment_ratio = len(submissions) / (len(comments) + 1)\n",
    "    link_count = sum(1 for c in comments if 'http' in c.body.lower())\n",
    "    link_in_comment_ratio = link_count / len(comments) if comments else 0\n",
    "    comment_bodies = [c.body for c in comments]\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = [analyzer.polarity_scores(comment)['compound'] for comment in comment_bodies]\n",
    "    sentiment_avg = np.mean(sentiment_scores) if sentiment_scores else 0\n",
    "    sentiment_variance = np.var(sentiment_scores) if sentiment_scores else 0\n",
    "    comment_similarity_avg = 0.0\n",
    "    if len(comment_bodies) > 1:\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english').fit_transform(comment_bodies)\n",
    "            cosine_matrix = cosine_similarity(vectorizer)\n",
    "            upper_triangle_indices = np.triu_indices_from(cosine_matrix, k=1)\n",
    "            if upper_triangle_indices[0].size > 0:\n",
    "                comment_similarity_avg = np.mean(cosine_matrix[upper_triangle_indices])\n",
    "        except ValueError:\n",
    "            comment_similarity_avg = 0.0\n",
    "    exact_duplicate_ratio = 0.0\n",
    "    if len(comment_bodies) > 0:\n",
    "        num_unique_comments = len(set(comment_bodies))\n",
    "        exact_duplicate_ratio = (len(comment_bodies) - num_unique_comments) / len(comment_bodies)\n",
    "    features = {\n",
    "        'account_age_days': account_age_days, 'karma_ratio': karma_ratio,\n",
    "        'avg_time_between_posts_sec': avg_time_between_posts_sec, 'subreddit_entropy': subreddit_entropy,\n",
    "        'username_is_pattern': username_is_pattern, 'comment_length_variance': comment_length_variance,\n",
    "        'submission_comment_ratio': submission_comment_ratio, 'link_in_comment_ratio': link_in_comment_ratio,\n",
    "        'sentiment_avg': sentiment_avg, 'sentiment_variance': sentiment_variance,\n",
    "        'comment_similarity_avg': comment_similarity_avg, 'exact_duplicate_ratio': exact_duplicate_ratio,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def analyze_with_llm(username, reddit_instance, metrics_data, model_name=\"gemini-1.5-flash\"):\n",
    "    \"\"\"Usa um LLM (Gemini) para analisar os dados de um usuário.\"\"\"\n",
    "    try:\n",
    "        user = reddit_instance.redditor(username)\n",
    "        comments = list(user.comments.new(limit=25))\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Não foi possível buscar os dados do usuário: {e}\"}\n",
    "\n",
    "    if len(comments) < 5:\n",
    "        prompt = f\"\"\"\n",
    "        Você é um analista especialista em detecção de bots na plataforma Reddit. Sua tarefa é fazer uma\n",
    "        análise do usuário u/{username} baseada APENAS nos seus dados quantitativos, pois\n",
    "        ele não possui comentários recentes suficientes para uma análise de texto.\n",
    "\n",
    "        --- DADOS QUANTITATIVOS ---\n",
    "        * Idade da Conta (dias): {metrics_data.get('account_age_days', 'N/A')} (Contas muito novas são suspeitas)\n",
    "        * Proporção de Karma (comentário/link): {metrics_data.get('karma_ratio', 0.0):.2f} (Valores extremos podem ser suspeitos)\n",
    "        * Taxa de Duplicação Exata de Comentários: {metrics_data.get('exact_duplicate_ratio', 0.0):.2f} (Uma taxa > 0.25 é forte sinal de bot)\n",
    "        * Tempo Médio entre Atividades (segundos): {metrics_data.get('avg_time_between_posts_sec', 0.0):.2f} (Valores muito baixos podem indicar automação)\n",
    "        * Entropia de Subreddits: {metrics_data.get('subreddit_entropy', 0.0):.2f} (Valores muito baixos indicam atividade concentrada)\n",
    "        * Variância do Tamanho dos Comentários: {metrics_data.get('comment_length_variance', 0.0):.2f} (Próximo de zero indica comentários de tamanho similar)\n",
    "        ---\n",
    "\n",
    "        Com base APENAS nos dados quantitativos acima, faça sua avaliação final.\n",
    "\n",
    "        Responda estritamente no formato JSON a seguir, sem texto adicional:\n",
    "        {{\n",
    "          \"veredicto\": \"bot\" | \"humano\",\n",
    "          \"confianca\": <um número de 0.0 a 1.0>,\n",
    "          \"justificativa\": \"<uma análise concisa de 2-3 frases explicando sua decisão com base APENAS nos dados numéricos. Mencione a ausência de comentários.>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        comment_list_for_prompt = \"\\\\n\".join([f\"{i+1}. {c.body.strip()}\" for i, c in enumerate(comments)])\n",
    "        prompt = f\"\"\"\n",
    "        Você é um analista especialista em detecção de bots na plataforma Reddit. Sua tarefa é fazer uma\n",
    "        análise híbrida, considerando tanto os dados quantitativos quanto o conteúdo dos comentários\n",
    "        recentes do usuário u/{username}.\n",
    "\n",
    "        --- DADOS QUANTITATIVOS ---\\n\n",
    "        * Idade da Conta (dias): {metrics_data.get('account_age_days', 'N/A')} (Contas muito novas são mais suspeitas)\\n\n",
    "        * Proporção de Karma (comentário/link): {metrics_data.get('karma_ratio', 0.0):.2f} (Valores extremos podem ser suspeitos)\\n\n",
    "        * Taxa de Duplicação Exata de Comentários: {metrics_data.get('exact_duplicate_ratio', 0.0):.2f} (Uma taxa acima de 0.25 é um forte sinal de bot)\n",
    "\n",
    "        --- COMENTÁRIOS RECENTES ({len(comments)}) ---\\n\n",
    "        {comment_list_for_prompt}\\n\n",
    "        ---\n",
    "\n",
    "        Com base em TODOS os dados acima (quantitativos e textuais), faça sua avaliação final.\n",
    "        Considere como os dados numéricos reforçam ou contradizem a análise do texto.\n",
    "\n",
    "        Responda estritamente no formato JSON a seguir, sem texto adicional:\n",
    "        {{\n",
    "          \"veredicto\": \"bot\" | \"humano\",\n",
    "          \"confianca\": <um número de 0.0 a 1.0>,\n",
    "          \"justificativa\": \"<uma análise concisa de 2-3 frases explicando sua decisão com base nos dados e nos comentários>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        cleaned_response = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        return json.loads(cleaned_response)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Erro ao analisar com o LLM: {e}\", \"raw_response\": response.text if 'response' in locals() else \"\"}\n",
    "\n",
    "\n",
    "# --- 3. GRADIO INTERFACE LOGIC ---\n",
    "# This cell contains the functions that will power the Gradio interface.\n",
    "\n",
    "def setup_apis(google_key, reddit_id, reddit_secret, reddit_agent):\n",
    "    \"\"\"Configura as APIs com as chaves fornecidas pela interface.\"\"\"\n",
    "    global reddit_instance, user_db, is_configured\n",
    "    \n",
    "    # Configurar Google AI\n",
    "    try:\n",
    "        genai.configure(api_key=google_key)\n",
    "    except Exception as e:\n",
    "        return f\"❌ Erro na configuração do Google AI: {e}\", gr.update(visible=False), gr.update(visible=False)\n",
    "\n",
    "    # Configurar Reddit PRAW\n",
    "    try:\n",
    "        reddit_instance = praw.Reddit(\n",
    "            client_id=reddit_id,\n",
    "            client_secret=reddit_secret,\n",
    "            user_agent=reddit_agent\n",
    "        )\n",
    "        if reddit_instance.read_only:\n",
    "            status_msg = \"✅ APIs configuradas (Reddit em modo somente leitura).\"\n",
    "        else:\n",
    "            status_msg = f\"✅ APIs configuradas (Reddit autenticado como u/{reddit_instance.user.me()}).\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Erro na configuração do Reddit PRAW: {e}\", gr.update(visible=False), gr.update(visible=False)\n",
    "\n",
    "    # Carregar banco de dados\n",
    "    user_db = load_database()\n",
    "    status_msg += f\"\\n💾 Banco de dados carregado com {len(user_db)} entradas.\"\n",
    "    is_configured = True\n",
    "    \n",
    "    # Retorna a mensagem de status e mostra os próximos elementos da UI\n",
    "    return status_msg, gr.update(visible=True), gr.update(visible=True)\n",
    "\n",
    "\n",
    "def run_full_analysis(url):\n",
    "    \"\"\"Função principal que é acionada pelo botão 'Analisar'.\"\"\"\n",
    "    if not is_configured or not reddit_instance:\n",
    "        return \"Erro: As APIs não foram configuradas. Por favor, preencha e salve suas credenciais primeiro.\", \"\", \"\", gr.update(visible=False), None\n",
    "\n",
    "    match = re.search(r\"/user/([A-Za-z0-9_-]+)\", url)\n",
    "    if not match:\n",
    "        return \"❌ URL inválida. Use o formato: https://www.reddit.com/user/username/\", \"\", \"\", gr.update(visible=False), None\n",
    "    \n",
    "    username = match.group(1)\n",
    "    \n",
    "    # 1. Análise de Métricas\n",
    "    status_log = f\"📡 1/2: Calculando métricas para u/{username}...\"\n",
    "    yield status_log, \"\", \"\", gr.update(visible=False), None\n",
    "    \n",
    "    features = analyze_user_live(username, reddit_instance)\n",
    "    if \"error\" in features:\n",
    "        error_msg = f\"❌ Erro na análise de métricas: {features['error']}\"\n",
    "        yield error_msg, \"\", \"\", gr.update(visible=False), None\n",
    "        return\n",
    "\n",
    "    metrics_md = \"### 📈 Métricas Quantitativas\\n---\\n\"\n",
    "    for key, value in features.items():\n",
    "        metrics_md += f\"- **{key.replace('_', ' ').title()}**: {value:.4f}\\n\"\n",
    "    \n",
    "    # 2. Análise com LLM\n",
    "    status_log += f\"\\n🧠 2/2: Solicitando análise do Gemini para u/{username}...\"\n",
    "    yield status_log, metrics_md, \"\", gr.update(visible=False), None\n",
    "    \n",
    "    llm_analysis = analyze_with_llm(username, reddit_instance, features)\n",
    "    \n",
    "    if \"error\" in llm_analysis:\n",
    "        error_msg = f\"❌ Erro na análise do LLM: {llm_analysis['error']}\"\n",
    "        ai_verdict_md = f\"### 🤖 Veredito da IA\\n---\\n{error_msg}\"\n",
    "        status_log += \"\\n❌ Análise falhou.\"\n",
    "        yield status_log, metrics_md, ai_verdict_md, gr.update(visible=False), None\n",
    "        return\n",
    "\n",
    "    verdict = llm_analysis.get('veredicto', 'N/A').upper()\n",
    "    confidence = llm_analysis.get('confianca', 0.0)\n",
    "    justification = llm_analysis.get('justificativa', 'N/A')\n",
    "\n",
    "    emoji = \"🤖\" if verdict == \"BOT\" else \"🧑\"\n",
    "    ai_verdict_md = f\"### {emoji} Veredito da IA\\n---\\n\"\n",
    "    ai_verdict_md += f\"**Veredito:** `{verdict}`\\n\\n\"\n",
    "    ai_verdict_md += f\"**Confiança:** `{confidence:.2%}`\\n\\n\"\n",
    "    ai_verdict_md += f\"**Justificativa:** *{justification}*\"\n",
    "\n",
    "    status_log += f\"\\n✅ Análise concluída! Veredito: {verdict}. Por favor, salve seu veredito final.\"\n",
    "\n",
    "    # Retorna todos os resultados e mostra os botões de salvar\n",
    "    yield status_log, metrics_md, ai_verdict_md, gr.update(visible=True), features\n",
    "\n",
    "\n",
    "def save_final_verdict(features, label):\n",
    "    \"\"\"Salva a entrada no banco de dados com o rótulo final do usuário.\"\"\"\n",
    "    global user_db\n",
    "    if features is None:\n",
    "        return \"Nada para salvar.\", f\"💾 Banco de dados: {len(user_db)} entradas.\"\n",
    "\n",
    "    is_bot_label = 1 if label == 'bot' else 0\n",
    "    features['is_bot'] = is_bot_label\n",
    "    \n",
    "    new_entry = pd.DataFrame([features])\n",
    "    user_db = pd.concat([user_db, new_entry], ignore_index=True)\n",
    "    user_db.to_csv(DATABASE_FILE, index=False)\n",
    "    \n",
    "    confirmation_msg = f\"✅ Veredito '{label.upper()}' salvo! O banco de dados agora tem {len(user_db)} entradas.\"\n",
    "    db_status_msg = f\"💾 Banco de dados: {len(user_db)} entradas. Salvo em '{DATABASE_FILE}'.\"\n",
    "    \n",
    "    return confirmation_msg, db_status_msg\n",
    "\n",
    "# --- 4. GRADIO UI DEFINITION ---\n",
    "# Esta célula constrói e executa a interface web.\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Reddit Bot Detector\") as demo:\n",
    "    gr.Markdown(\"# 🤖 Reddit Bot Detector com IA Híbrida\")\n",
    "    gr.Markdown(\"Uma ferramenta para analisar perfis de usuários do Reddit usando métricas quantitativas e análise da IA do Google (Gemini).\")\n",
    "\n",
    "    # Estado para armazenar os 'features' entre a análise e o salvamento\n",
    "    analysis_features = gr.State(value=None)\n",
    "    \n",
    "    with gr.Accordion(\"🔑 Configuração de APIs (Obrigatório)\", open=True) as setup_accordion:\n",
    "        with gr.Row():\n",
    "            google_key_input = gr.Textbox(label=\"Google AI API Key\", type=\"password\")\n",
    "        with gr.Row():\n",
    "            reddit_id_input = gr.Textbox(label=\"Reddit Client ID\", type=\"password\")\n",
    "            reddit_secret_input = gr.Textbox(label=\"Reddit Client Secret\", type=\"password\")\n",
    "            reddit_agent_input = gr.Textbox(label=\"Reddit User Agent\", value=\"SubredditAnalyzer/1.0 by YourUsername\")\n",
    "        setup_button = gr.Button(\"Salvar Credenciais e Iniciar\")\n",
    "    \n",
    "    status_log_box = gr.Textbox(label=\"Status\", interactive=False, lines=2)\n",
    "    \n",
    "    # ❗️ CORREÇÃO: Trocado gr.Box por gr.Group\n",
    "    with gr.Group(visible=False) as analysis_box:\n",
    "        with gr.Row():\n",
    "            url_input = gr.Textbox(label=\"URL do Perfil do Usuário Reddit\", placeholder=\"https://www.reddit.com/user/SomeUser\")\n",
    "            analyze_button = gr.Button(\"🔍 Analisar\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            metrics_output = gr.Markdown(label=\"Métricas Quantitativas\")\n",
    "            ai_verdict_output = gr.Markdown(label=\"Veredito da IA\")\n",
    "    \n",
    "    # ❗️ CORREÇÃO: Trocado gr.Box por gr.Group\n",
    "    with gr.Group(visible=False) as verdict_box:\n",
    "        gr.Markdown(\"--- \\n### 👉 Qual é o seu veredito final?\")\n",
    "        gr.Markdown(\"Com base nos dados acima, classifique o usuário. Sua resposta treinará o modelo futuro.\")\n",
    "        with gr.Row():\n",
    "            save_bot_button = gr.Button(\"CONFIRMAR COMO BOT 🤖\")\n",
    "            save_human_button = gr.Button(\"CONFIRMAR COMO HUMANO 🧑\")\n",
    "        db_status_box = gr.Textbox(label=\"Status do Banco de Dados\", interactive=False)\n",
    "\n",
    "    # Lógica dos Eventos da UI\n",
    "    setup_button.click(\n",
    "        fn=setup_apis,\n",
    "        inputs=[google_key_input, reddit_id_input, reddit_secret_input, reddit_agent_input],\n",
    "        outputs=[status_log_box, analysis_box, verdict_box]\n",
    "    )\n",
    "    \n",
    "    analyze_button.click(\n",
    "        fn=run_full_analysis,\n",
    "        inputs=[url_input],\n",
    "        outputs=[status_log_box, metrics_output, ai_verdict_output, verdict_box, analysis_features]\n",
    "    )\n",
    "    \n",
    "    save_bot_button.click(\n",
    "        fn=lambda features: save_final_verdict(features, 'bot'),\n",
    "        inputs=[analysis_features],\n",
    "        outputs=[status_log_box, db_status_box]\n",
    "    )\n",
    "    \n",
    "    save_human_button.click(\n",
    "        fn=lambda features: save_final_verdict(features, 'human'),\n",
    "        inputs=[analysis_features],\n",
    "        outputs=[status_log_box, db_status_box]\n",
    "    )\n",
    "\n",
    "# Inicia a interface\n",
    "demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
